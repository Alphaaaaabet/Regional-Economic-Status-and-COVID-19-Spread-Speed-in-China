{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COGS 108 - Final Project "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Permissions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Place an `X` in the appropriate bracket below to specify if you would like your group's project to be made available to the public. (Note that PIDs will be scraped from the public submission, but student names will be included.)\n",
    "\n",
    "* [  ] YES - make available\n",
    "* [ x ] NO - keep private"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Fill in your overview here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Names\n",
    "\n",
    "- Yang Li\n",
    "- Yiou Lyu\n",
    "- Linfeng Hu\n",
    "- Ruby Celeste Marroquin "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Group Members IDs\n",
    "\n",
    "- A15560579\n",
    "- A15930345\n",
    "- A15473121\n",
    "- A16094382"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Research Question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does the regional economic status of each province in mainland China correlate to its breakout and recovery of COVID-19?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background and Prior Work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Fill in your background and prior work here* \n",
    "\n",
    "References (include links):\n",
    "- 1)\n",
    "- 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hypothesis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Fill in your hypotheses here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Copy this information for each dataset)\n",
    "- Dataset Name: \n",
    "- Link to the dataset:\n",
    "- Number of observations:\n",
    "\n",
    "1-2 sentences describing each dataset. \n",
    "\n",
    "If you plan to use multiple datasets, add 1-2 sentences about how you plan to combine these datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import codecs\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import os\n",
    "import patsy\n",
    "import scipy.stats as stats\n",
    "\n",
    "import bs4\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with codecs.open('Data/virus.json', 'r', 'utf-8') as data_file:\n",
    "    #data_teacher = json.load(data_file, 'utf-8')\n",
    "\n",
    "#topic[worksheet] = data_teacher[worksheetID]['Topic']\n",
    "#out = codecs.open('Worksheet.csv', 'w', 'utf-8')\n",
    "#out.write(topic[worksheet])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To clean our data, our general approach is to represent datasets in pandas dataframe. Then we drop irrelevant information or outliers in data. We also rename the columns to make it easier for later analyses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, we deal with the datasets that consist of economic status data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the income per capita value for each province in mainland China. Income is measured in yuan. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File b'Data/Income.csv' does not exist: b'Data/Income.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-09827233a4e9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mIncome\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Data/Income.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mIncome\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mIncome\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhow\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'all'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mIncome\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\Ruby\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    683\u001b[0m         )\n\u001b[0;32m    684\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 685\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    686\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    687\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\Ruby\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    455\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    456\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 457\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    458\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    459\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\Ruby\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    893\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    894\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 895\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    896\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    897\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\Ruby\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1133\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"c\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1134\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"c\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1135\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1136\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1137\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"python\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\Ruby\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1915\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"usecols\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1916\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1917\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1918\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1919\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] File b'Data/Income.csv' does not exist: b'Data/Income.csv'"
     ]
    }
   ],
   "source": [
    "Income = pd.read_csv('Data/Income.csv')\n",
    "Income = Income.dropna(axis=1, how='all')\n",
    "Income.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the per capita Gross Regional Product value for each province. GRP per capita is measured in yuan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GRP = pd.read_csv('Data/GRP.csv')\n",
    "GRP = GRP.dropna(axis=1, how='all')\n",
    "GRP.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we move on to clean the population density related datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Population per province here is calculated in the unit of 10000 persons). It includes all residents (permanent and temporary, rural and urban)at the end of that year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "population = pd.read_csv('Data/Population.csv')\n",
    "population = population.dropna(axis = 1, how = 'all')\n",
    "population.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To calculate population density of a region, we also need to areas of each province. Here, area of each province is measured in unit of square kilometers.\n",
    "\n",
    "Since we only need the area information of each separate region, we will drop the \"Toal\" row at the end which contains information about the total area of China(judging by the data contained, the row name should be a typo)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "area = pd.read_csv('Data/Area.csv')\n",
    "area = area.dropna(axis = 1, how = 'all')\n",
    "#shorten column names to make following analysis simpler\n",
    "area = area.rename(columns={\"Area (sq.km)\": \"Area\"})\n",
    "area = area[area.District != 'Toal']\n",
    "area.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read virus data into dataframes \n",
    "\n",
    "list_of_virus_data = list()\n",
    "\n",
    "# append data between Feb 1 and Feb 25 to list\n",
    "for i in range(20200201,20200226): \n",
    "    path = './Data/virus/' + str(i) + '.csv'\n",
    "    list_of_virus_data.append(pd.read_csv(path))\n",
    "    \n",
    "# File 20200226.csv is missing, reason unknow. \n",
    "\n",
    "    \n",
    "# append data between Feb 27 and Feb 29 to list\n",
    "for i in range(20200227,20200230): \n",
    "    path = './Data/virus/' + str(i) + '.csv'\n",
    "    list_of_virus_data.append(pd.read_csv(path))\n",
    "\n",
    "# append data between Mar 1 and  Mar 1 to list\n",
    "for i in range(20200301,20200302): \n",
    "    path = './Data/virus/' + str(i) + '.csv'\n",
    "    list_of_virus_data.append(pd.read_csv(path))\n",
    "    \n",
    "print('number of dataframes for virus: ',len(list_of_virus_data))\n",
    "\n",
    "# access ith elment in the list using list_of_virus_data[i]\n",
    "# for example list_of_virus_data[0] gives the first dataframe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start cleaning virus data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean 0th to 1th df in the list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean 0th to 1th df in the list \n",
    "for i in range(0,2):\n",
    "    # get the df of the ith day\n",
    "    df = list_of_virus_data[i]\n",
    "    # use the first data row as column names\n",
    "    df.columns = df.iloc[0]\n",
    "    # drop first row, because is was used as header\n",
    "    df = df.drop(0)\n",
    "    # drop the column '1', because it is irrelevant\n",
    "    df = df.drop(1, axis=1)\n",
    "    # save cleaned data to list_of_virus_data \n",
    "    list_of_virus_data[i] = df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean 2th df in the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Clean 2th df in the list\n",
    "# get the df of the ith day\n",
    "df = list_of_virus_data[2]\n",
    "# reset column names\n",
    "df.columns = [\"Province/Region/City\", \"Confirmed Cases\", 1]\n",
    "# drop meaningless 1\" column,  keep \"Confirmed Cases\" and \"Province/Region/City\"\n",
    "df = df.drop(1, axis=1)\n",
    "# Drop the last row, because it is comment instaed of data\n",
    "df = df.drop(df.shape[0] - 1)\n",
    "# save cleaned data to list_of_virus_data \n",
    "list_of_virus_data[2] = df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean 3th to 10th df in the list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean 3th to 10th df in the list \n",
    "for i in range(3,11):\n",
    "    # get the df of the ith day\n",
    "    df = list_of_virus_data[i]\n",
    "    # use the first data row as column names\n",
    "    df.columns = df.iloc[0]\n",
    "    # drop first row, because is was used as header\n",
    "    df = df.drop(0)\n",
    "    # drop the column '1', because it is irrlavent\n",
    "    df = df.drop(1, axis=1)\n",
    "    # save cleaned data to list_of_virus_data\n",
    "    list_of_virus_data[i] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# View virus data of the first 11 days\n",
    "# Delete this cell\n",
    "for i in range (0,11):\n",
    "    print(list_of_virus_data[i])\n",
    "    print(\"\\n\")\n",
    "    print(\"\\n\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Clean 11th to the 28th df in the list \n",
    "# TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_columns(df):\n",
    "    \n",
    "    #Get a list of the columns \n",
    "    data = list(df.columns)\n",
    "    \n",
    "    #Drops the number of records \n",
    "    if(len(data) <= 4):\n",
    "        df['Suspected Cases'] = 0\n",
    "        df['Deaths'] = 0\n",
    "        df.drop(df.columns[3], axis=1, inplace=True)\n",
    "    if(len(data) > 4):\n",
    "        df.drop(df.columns[2], axis=1, inplace=True)\n",
    "        df.drop(df.columns[len(data)-1], axis=1, inplace=True)\n",
    "        df['Suspected Cases'] = 0\n",
    "        df['Deaths'] = 0\n",
    "    \n",
    "    df = df[['Date', 'Province/Region/City', 'Confirmed Cases',\n",
    "                 'Suspected Cases', 'Deaths']]\n",
    "    \n",
    "    df.drop(df.columns[0], axis=1, inplace=True)\n",
    "    df.drop(df.tail(1).index,inplace=True)\n",
    "\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_columns_2(df):\n",
    "    \n",
    "    #Get a list of the columns \n",
    "    data = list(df.columns)\n",
    "    \n",
    "    #drops the number of records column \n",
    "    if \"Number of Records\" in data:\n",
    "        df.drop(df.columns[len(data)-1], axis=1, inplace=True)\n",
    "    if(len(data) >= 12):\n",
    "        df.drop(df.columns[[2,3, 4, 5, 7, 9,10]], axis=1, inplace=True)\n",
    "        if \"Lab-confirmed 1\" in data:\n",
    "            df.rename(columns={\"Lab-confirmed 1\":'Confirmed Cases'}, inplace=True)\n",
    "        if \"Deaths 1\" in data: \n",
    "            df.rename(columns={\"Deaths 1\":'Deaths'}, inplace=True)\n",
    "    if(len(data) < 12):\n",
    "        #used to remove population \n",
    "        if \"Population (in 10,000s)\" in data:\n",
    "            df.drop(df.columns[2], axis=1, inplace=True)\n",
    "        \n",
    "        #Renames total deaths to deaths \n",
    "        if \"Total Deaths\" in data:\n",
    "            df.rename(columns={'Total Deaths':'Deaths'}, inplace=True)\n",
    "    \n",
    "        #Checks to see if suspected cases and/or death exists or else creates\n",
    "        #and fills with 0 since no data was available at the time \n",
    "        if not \"Suspected Cases\" in data:\n",
    "            df['Suspected Cases'] = 0\n",
    "        if not \"Deaths\" in data:\n",
    "            df['Deaths'] = 0\n",
    "    \n",
    "    #Drops any remaining columns that aren't any of the following\n",
    "    df = df[['Date', 'Province/Region/City', 'Confirmed Cases',\n",
    "             'Suspected Cases', 'Deaths']]\n",
    "    df.drop(df.columns[0], axis=1, inplace=True)\n",
    "    df.drop(df.tail(1).index,inplace=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_columns_3(df):\n",
    "    #Get a list of the columns \n",
    "    data = list(df.columns)\n",
    "    \n",
    "    #deletes the number of records column \n",
    "    del df['Number of Records']\n",
    "    \n",
    "    if \"F5\" in data:\n",
    "        del df['F5']\n",
    "    if \"F3\" in data:\n",
    "        del df['F3']\n",
    "    if \"F2\" in data:\n",
    "        del df['F2']\n",
    "    if 'Population' in data:\n",
    "        del df['Population']\n",
    "    if 'Population (in 10,000s)' in data:\n",
    "        del df['Population (in 10,000s)']\n",
    "    if 'F1' in data:\n",
    "        df.rename(columns={'F1':'Province/Region/City'}, inplace=True)\n",
    "    if 'F4' in data:\n",
    "        df.rename(columns={'F4':'Suspected Cases'}, inplace=True)\n",
    "    if 'F6' in data:\n",
    "        df.rename(columns={'F6':'Confirmed Cases'}, inplace=True)\n",
    "    if 'F7' in data:\n",
    "        df.rename(columns={'F7':'Deaths'}, inplace=True)\n",
    "    \n",
    "    df = df[['Province/Region/City', 'Confirmed Cases',\n",
    "        'Suspected Cases', 'Deaths']]\n",
    "    \n",
    "    df.drop(df.index[[0, 1, 2, 3]], inplace=True)\n",
    "    \n",
    "    df.drop(df.tail(1).index,inplace=True)\n",
    "    df.dropna(how='any')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_to_read = [\"Feb1_Data.csv\", \"Feb2_Data.csv\", \"Feb3_Data.csv\", \"Feb4_Data.csv\", \"Feb5_Data.csv\",\n",
    "                 \"Feb6_Data.csv\", \"Feb7_Data.csv\", \"Feb8_Data.csv\", \"Feb9_Data.csv\", \"Feb10_Data.csv\", \n",
    "                 \"Feb11_Data.csv\", \"Feb12_Data.csv\", \"Feb13_Data.csv\", \"Feb14_Data.csv\", \"Feb15_Data.csv\",\n",
    "                 \"20200216.csv\", \"20200217.csv\", \"20200218.csv\", \"20200219.csv\", \"20200220.csv\", \n",
    "                 \"20200221.csv\", \"20200222.csv\", \"20200223.csv\", \"20200224.csv\", \"20200225.csv\", \"20200227.csv\",\n",
    "                 \"20200228.csv\", \"20200229.csv\", \"20200301.csv\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creates a new DataFrame with the first file and used to concat the rest \n",
    "df = pd.read_csv(\"Feb1_Data.csv\")\n",
    "df = fix_columns(df)\n",
    "\n",
    "for i in range(0, 10):\n",
    "    next_file = pd.read_csv(files_to_read[i])\n",
    "    next_file = fix_columns(next_file)\n",
    "    df = pd.concat([df, next_file], axis = 0, join = 'inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(11, 15):\n",
    "    next_file = pd.read_csv(files_to_read[i])\n",
    "    next_file = fix_columns_2(next_file)\n",
    "    df = pd.concat([df, next_file], axis = 0, join = 'inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(16, 29):\n",
    "    next_file = pd.read_csv(files_to_read[i])\n",
    "    if(i == 21):\n",
    "        next_file.columns = ['Population', 'F3', 'Suspected Cases', 'F5', 'Confirmed Cases', 'Deaths', \n",
    "                     'Number of Records', 'Province/Region/City']\n",
    "        next_file = fix_columns_3(next_file)\n",
    "    elif (i == 25):\n",
    "        next_file.columns = ['Population', 'F3', 'Suspected Cases', 'F5','Confirmed Cases', 'Deaths', \n",
    "                     'Number of Records', 'Province/Region/City']\n",
    "        next_file = fix_columns_3(next_file)\n",
    "    else:    \n",
    "        next_file = fix_columns_3(next_file)\n",
    "    \n",
    "    df = pd.concat([df, next_file], axis = 0, join = 'inner')\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Analysis & Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Include cells that describe the steps in your data analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR CODE HERE\n",
    "## FEEL FREE TO ADD MULTIPLE CELLS PER SECTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ethics & Privacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Fill in your ethics & privacy discussion here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion & Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Fill in your discussion information here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Team Contributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Specify who in your group worked on which parts of the project.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
